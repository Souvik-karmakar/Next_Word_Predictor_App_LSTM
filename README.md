# üß† Next Word Prediction Using LSTM

## üìå Project Overview
This project implements a **Next Word Prediction model** using a **Long Short-Term Memory (LSTM)** neural network to predict the most probable next word in a given text sequence. The model is trained on **Shakespeare‚Äôs _Hamlet_**, enabling it to learn complex linguistic patterns and contextual relationships.

The project covers the complete **end-to-end NLP pipeline**, including data collection, preprocessing, model training with early stopping, evaluation, and deployment using **Streamlit** for real-time predictions.

---

## üéØ Objective
- Predict the next word in a given sentence using sequence modeling  
- Learn contextual word relationships using LSTM-based RNNs  
- Deploy an interactive web application for real-time inference  

---

## üìÇ Dataset
- **Source:** NLTK Gutenberg Corpus  
- **Text:** Shakespeare ‚Äì _Hamlet_  
- The dataset provides rich vocabulary and complex sentence structures for effective language modeling.

---
## App Screenshot
![App Screenshot](LSTM_Next_Word_Prediction_App.png)

---

## ‚öôÔ∏è Project Workflow

### 1Ô∏è‚É£ Data Collection
- Loaded Shakespeare‚Äôs *Hamlet* using NLTK‚Äôs Gutenberg corpus  
- Stored raw text locally for preprocessing  

### 2Ô∏è‚É£ Data Preprocessing
- Converted text to lowercase  
- Tokenized text using **Keras Tokenizer**  
- Generated n-gram sequences  
- Applied padding to ensure uniform input length  
- Split data into training and testing sets  

---

### 3Ô∏è‚É£ Model Architecture

#### üîπ LSTM Model
- Embedding Layer  
- Two stacked LSTM layers  
- Dropout for regularization  
- Dense layer with Softmax activation  

#### üîπ GRU Model (Experimental)
- Implemented GRU-based RNN for comparison with LSTM  

---

### 4Ô∏è‚É£ Model Training
- **Loss Function:** Categorical Crossentropy  
- **Optimizer:** Adam  
- **Metric:** Accuracy  
- **Early Stopping** applied to prevent overfitting by monitoring validation loss  

---

### 5Ô∏è‚É£ Model Evaluation
- Tested the model using unseen text sequences  
- Predicted next words based on learned contextual patterns  

---

### 6Ô∏è‚É£ Deployment
- Built an interactive **Streamlit application**  
- Users can input a sequence of words and receive real-time predictions  
- Model and tokenizer are loaded for inference
- Live App Link :- https://nextwordpredictorapplstm-b48qlgs2lekfte4pqfmzbb.streamlit.app/

---

## üß™ Example Prediction
```text
Input:  To be or not to be
Output: that
