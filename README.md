# ğŸ§  Next Word Prediction Using LSTM

## ğŸ“Œ Project Overview
This project implements a **Next Word Prediction model** using a **Long Short-Term Memory (LSTM)** neural network to predict the most probable next word in a given text sequence. The model is trained on **Shakespeareâ€™s _Hamlet_**, enabling it to learn complex linguistic patterns and contextual relationships.

The project covers the complete **end-to-end NLP pipeline**, including data collection, preprocessing, model training with early stopping, evaluation, and deployment using **Streamlit** for real-time predictions.

---

## ğŸ¯ Objective
- Predict the next word in a given sentence using sequence modeling  
- Learn contextual word relationships using LSTM-based RNNs  
- Deploy an interactive web application for real-time inference  

---

## ğŸ“‚ Dataset
- **Source:** NLTK Gutenberg Corpus  
- **Text:** Shakespeare â€“ _Hamlet_  
- The dataset provides rich vocabulary and complex sentence structures for effective language modeling.

---
## App Screenshot
![App Screenshot](screenshots/app_ui.png)

---

## âš™ï¸ Project Workflow

### 1ï¸âƒ£ Data Collection
- Loaded Shakespeareâ€™s *Hamlet* using NLTKâ€™s Gutenberg corpus  
- Stored raw text locally for preprocessing  

### 2ï¸âƒ£ Data Preprocessing
- Converted text to lowercase  
- Tokenized text using **Keras Tokenizer**  
- Generated n-gram sequences  
- Applied padding to ensure uniform input length  
- Split data into training and testing sets  

---

### 3ï¸âƒ£ Model Architecture

#### ğŸ”¹ LSTM Model
- Embedding Layer  
- Two stacked LSTM layers  
- Dropout for regularization  
- Dense layer with Softmax activation  

#### ğŸ”¹ GRU Model (Experimental)
- Implemented GRU-based RNN for comparison with LSTM  

---

### 4ï¸âƒ£ Model Training
- **Loss Function:** Categorical Crossentropy  
- **Optimizer:** Adam  
- **Metric:** Accuracy  
- **Early Stopping** applied to prevent overfitting by monitoring validation loss  

---

### 5ï¸âƒ£ Model Evaluation
- Tested the model using unseen text sequences  
- Predicted next words based on learned contextual patterns  

---

### 6ï¸âƒ£ Deployment
- Built an interactive **Streamlit application**  
- Users can input a sequence of words and receive real-time predictions  
- Model and tokenizer are loaded for inference  

---

## ğŸ§ª Example Prediction
```text
Input:  To be or not to be
Output: that
